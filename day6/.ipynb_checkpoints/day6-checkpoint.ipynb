{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2368bcc9-bb3f-4e6c-829d-7cab0dd94691",
   "metadata": {},
   "source": [
    "# Day 6 - Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395ba6ac-25d6-4a10-8bb8-8563e3a85018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "876c377f-3ea2-40b0-9382-0482f7223c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/efeacikgoz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/efeacikgoz/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('punkt')\n",
    "download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babe7f4b-fa67-4c1b-92e3-0239a7b5c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"Yaylaya gitmişti yayla zamanı, Gülizar döndü de Döndü dönmedi.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec780634-2459-4f8d-9378-badc4c97ee28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yaylaya',\n",
       " 'gitmişti',\n",
       " 'yayla',\n",
       " 'zamanı,',\n",
       " 'Gülizar',\n",
       " 'döndü',\n",
       " 'de',\n",
       " 'Döndü',\n",
       " 'dönmedi.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb47d1-f29c-4d20-9de0-7b03cc847e66",
   "metadata": {},
   "source": [
    "## Tokenize - split the words of a text into a python array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146108d-1436-4f0d-a47d-9c77fee9b13f",
   "metadata": {},
   "source": [
    "sent_tokenize tokenizes sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68fe123b-f335-4118-950d-6ab8bfbc5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f33a911-972f-4860-9821-2ef9d95b1625",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Welcome readers. I hope you find it interesting. Please do reply.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf4cba6-4364-4330-9c09-ae23dc2974de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1eb4807-2c41-4ea6-9bbc-59f98b0c9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize as wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa77a8c2-d089-44f1-b0fa-5b9fbe2a717a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'readers',\n",
       " '.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'it',\n",
       " 'interesting',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'reply',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf5af39-24b2-421d-9c96-6c1201c35a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ead443-b6a3-4418-8b1a-75fcbcec3cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'readers.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'it',\n",
       " 'interesting.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'reply',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TreebankWordTokenizer()\n",
    "tknzr.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920e735-64d5-445f-b73b-0e23239a930d",
   "metadata": {},
   "source": [
    "This tokenizer does not include the dots in the midlle of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "828c7f67-5041-4eae-9601-dfa5b725211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931d69af-6322-4943-8418-878ad81d3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "847a8a9c-8b99-451d-a5bf-973dc68dae9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', \"'\", 't', 'heistate', 'to', 'ask', 'questions', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.tokenize(\"Don't heistate to ask questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a09340b-d19c-4295-893a-a45107f43a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1911af8-7aff-468b-ba0c-5439e43788cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "blt = BlanklineTokenizer()\n",
    "\n",
    "sent = '''\n",
    "Hello miss\n",
    "\n",
    "I saw that I got an unexpected grade from the midterm. I would like to know if I can see my paper.\n",
    "\n",
    "with respect\n",
    "student\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b029e251-6a56-4772-b9b6-c578d7be5185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nHello miss',\n",
       " 'I saw that I got an unexpected grade from the midterm. I would like to know if I can see my paper.',\n",
       " 'with respect\\nstudent\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blt.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "225d651e-37e1-4d81-abd8-bdc3302c1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4494231b-ab61-4a92-982a-2d71fe806161",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"She secures 90.56% in class X. She is a meritorious student.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "931dc37e-868b-4bae-b4e5-e45ceb591691",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = RegexpTokenizer('[A-Z]\\w+') # Regex that matches all the words that start with a capital letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c52e168-6095-4d00-86fb-61cfb805596f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'She']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn.tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ceb72-987a-495f-a489-fc3dbc75a159",
   "metadata": {},
   "source": [
    "### Lemma & Stemma\n",
    "\n",
    "Remove the prefixes and find the stem of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71917de-3625-46b1-93ae-367298797475",
   "metadata": {},
   "source": [
    "#### Stemma: Remove the prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29169680-a2ff-4cdc-a472-ffbfb6967b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aec416a5-2cca-4cd6-a23e-9eb425ea9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbdb5118-e322-4248-9332-ebdbcb5a4519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.stem('talking')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3bd8b-be12-47b6-a096-c2febde0c4e8",
   "metadata": {},
   "source": [
    "Removed the 'ing' prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d9a8ec6-87aa-4ea3-923a-306b1564daa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d21ae278-a654-4daa-9beb-8f9cae37045b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geliyorlar'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.stem('geliyorlar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0115a-d70d-494a-b5a9-bd024efefb32",
   "metadata": {},
   "source": [
    "Did not work on a turkish word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa257d3d-97b0-4ebc-8786-60f44bc47792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcom'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.stem('welcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f5272f8-2b74-4df5-a36e-1d85950ccf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    'houses', 'trains', 'pens', 'cars', 'eaten', 'sick', 'bought', 'selling', 'sized', 'speech', 'rolling', 'marching',\n",
    "    'identification', 'universal', 'beautiful', 'references'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d5ce06a-f940-418a-aebe-7427e46f01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [pr.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7ea7f06-f583-463a-a0a3-c91761641fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hous',\n",
       " 'train',\n",
       " 'pen',\n",
       " 'car',\n",
       " 'eaten',\n",
       " 'sick',\n",
       " 'bought',\n",
       " 'sell',\n",
       " 'size',\n",
       " 'speech',\n",
       " 'roll',\n",
       " 'march',\n",
       " 'identif',\n",
       " 'univers',\n",
       " 'beauti',\n",
       " 'refer']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3df5d-27f1-4a37-b587-1ddd329fb82a",
   "metadata": {},
   "source": [
    "#### Lemma: Find the stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de08694a-0d81-45fd-8667-77f6c02b7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af2df4c3-e212-42b7-b9c2-9fe23799e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01ed702a-e5fe-4a61-9357-b9a18d6ee295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'working'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('working') # Needs nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "302b10a9-deb1-4eb9-9126-01d6239c6459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize(pr.stem('happiness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac48465-3a9d-429b-b6e5-606bcae0f769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
